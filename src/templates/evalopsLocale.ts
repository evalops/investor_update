export const evalopsLocale = {
  summary: {
    positive: [
      "EvalOps continues to execute strongly on our mission to transform ML evaluation workflows.",
      "Strong momentum in the evaluation infrastructure space with growing enterprise adoption.",
      "Our evaluation platform is becoming essential infrastructure for AI teams.",
      "Positive trends in usage metrics and customer satisfaction across our evaluation tools."
    ],
    neutral: [
      "EvalOps maintains steady progress on key evaluation platform initiatives.",
      "Consistent execution on our evaluation infrastructure roadmap.",
      "Balanced growth across core evaluation metrics and operational efficiency."
    ],
    negative: [
      "EvalOps faces some headwinds but remains focused on long-term evaluation platform success.",
      "Mixed results this period, with adjustments needed in our evaluation strategy.",
      "Some challenges in execution, but core evaluation platform fundamentals remain strong."
    ]
  },

  highlights: [
    "Successfully onboarded {newCustomers} new enterprise customers to our evaluation platform",
    "Processed {evalRuns} evaluation runs this month, up {growthRate}% from last month",
    "Expanded evaluation capabilities with new model comparison features",
    "Achieved {uptimePercent}% uptime across all evaluation infrastructure",
    "Reduced average evaluation runtime by {improvementPercent}% through compute optimizations",
    "Launched automated evaluation workflows for continuous model validation"
  ],

  concerns: [
    "Compute costs growing faster than planned due to increased evaluation workloads",
    "Customer acquisition velocity below targets in the enterprise evaluation space",
    "Competition intensifying in the ML evaluation and monitoring market",
    "Technical debt accumulating in legacy evaluation infrastructure",
    "Runway concerns if burn rate continues at current pace"
  ],

  asks: [
    "Introductions to ML platform leads at mid-market and enterprise companies",
    "Connections with VPs of Engineering at AI-first companies needing evaluation tools",
    "Referrals to CTOs implementing MLOps and model validation workflows",
    "Introductions to investors familiar with AI infrastructure and evaluation platforms",
    "Connections with technical advisors experienced in ML tooling and developer platforms"
  ],

  metricLabels: {
    evalRuns: "Evaluation Runs",
    activeWorkspaces: "Active Workspaces",
    computeSpend: "Compute Spend",
    grossMargin: "Gross Margin",
    averageEvalDuration: "Avg Eval Duration",
    pipelineArr: "Pipeline ARR",
    bookedArr: "Booked ARR"
  },

  chartTitles: {
    computeSpend: "GPU vs CPU Compute Spend",
    evalMetrics: "Evaluation Runs vs Revenue",
    runwayScenarios: "Runway: Current vs +20% Growth",
    evaluationTrends: "Monthly Evaluation Volume Trends"
  }
};
